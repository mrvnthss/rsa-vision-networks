defaults:
  - criterion: cross_entropy
  - dataset: fashionmnist
  - model: lenet
  - optimizer: sgd
  - _self_
  - experiment: ???
paths:
  checkpoints: ../out/checkpoints
  data: ../data
  logs: ../out/logs
  tensorboard: ../out/tensorboard
hydra:
  run:
    dir: ${paths.logs}/${experiment.subdir}
experiment:
  name: ???
  subdir: ${experiment.name}/${now:%Y-%m_%d-%H-%M-%S}
seeds:
  torch: ???
  shuffle: ???  # controls shuffling of the training set
  split: 42  # controls random splitting of the dataset into training and validation sets
dataloader:
  val_split: 0.15  # fraction of the training set to use as validation set
  batch_size: ???
  num_workers: ???
training:
  num_epochs: 90
  resume_from: null  # checkpoint to resume training from, null to start training from scratch
metrics:  # pass empty dict ``{}`` to disable tracking of additional metrics
  Accuracy@1:
    _target_: torchmetrics.classification.MulticlassAccuracy
    num_classes: ${dataset.num_classes}
    top_k: 1
    average: micro
    multidim_average: global
  Accuracy@5:
    _target_: torchmetrics.classification.MulticlassAccuracy
    num_classes: ${dataset.num_classes}
    top_k: 5
    average: micro
    multidim_average: global
performance:
  metric: Loss  # either "Loss" or one of the keys in the ``metrics`` dict
  higher_is_better: False
  dataset: Val  # either "Train" or "Val"
  patience: 10  # null to disable early stopping
  keep_previous_best_score: True
checkpoints:
  dir: ${paths.checkpoints}/${experiment.subdir}
  save_frequency: 10  # null to disable regular checkpoint saves
  save_best_model: True
  delete_previous: True
tensorboard:
  dir:  ${paths.tensorboard}/${experiment.subdir}
  updates_per_epoch:
    Train: 10  # null to disable TensorBoard logging during training
    Val: 10  # null to disable TensorBoard logging during validation
