# @package _global_

# This configuration file can be used to perform hyperparameter tuning
# using the Optuna hyperparameter optimization framework in order to
# train the LeNet-5 model on the FashionMNIST dataset.
#
# To run this experiment, execute the following command from the
# "rsa-vision-networks/src/" directory:
#
#   >>> python train_classifier_optuna.py experiment=lenet_fashionmnist/hparam_tuning/optuna/step_lr_random
#


# --------------------
# OPTUNA CONFIGURATION
# --------------------

optuna:
  n_trials: 100
  params:
    - name: optimizer.lr
      type: float
      vals:
        low: 1e-4
        high: 1e-1
        log: true
    - name: dataloader.batch_size
      type: categorical
      vals:
        choices: [32, 64, 128, 256, 512]
    - name: optimizer.momentum
      type: float
      vals:
        low: 0.0
        high: 0.95
    - name: optimizer.weight_decay
      type: float
      vals:
        low: 1e-6
        high: 1e-2
        log: true
    - name: transform.train.crop_scale.lower
      type: float
      vals:
        low: 0.08
        high: 1.0
    - name: transform.train.crop_ratio.lower
      type: float
      vals:
        low: 0.75
        high: 1.0
    - name: main_scheduler.lr_step_size
      type: int
      vals:
        low: 10
        high: 50
    - name: main_scheduler.lr_gamma
      type: float
      vals:
        low: 0.1
        high: 1.0


# ----------------------
# TRAINING CONFIGURATION
# ----------------------

defaults:
  - override /model: lenet
  - override /dataset: fashionmnist
  - override /transform/train: basic_augmentation
  - override /main_scheduler: step_lr
  - override /optuna/sampler: random
  - override /optuna/pruner: null  # disable pruning
  - _self_

experiment:
  name: lenet_fashionmnist/hparam_tuning/optuna/step_lr_random

reproducibility:
  torch_seed: 89
  optuna_seed: 123
  shuffle_seed: 858

training:
  num_epochs: 500

transform:
  val:
    resize_size: ${model.input_size}

dataloader:
  val_split: 0.20
  num_workers: 4

performance:
  min_delta: 1e-2  # 0 in "grid_search.yaml" and prev. experiments
  patience: 20  # same as in "grid_search.yaml"
