# @package _global_

# This configuration file can be used to run a grid search experiment to
# find a suitable learning rate schedule (step size & decay parameter of
# the StepLR scheduler) for training the LeNet-5 model on the
# FashionMNIST dataset.
#
# To run this experiment, execute the following command from the
# "thesis-experiments/src/" directory:
#
#   >>> python train_classifier.py experiment=lenet_fashionmnist/grid_search/lr_schedule
#


# Multi-run configuration
experiment:
  name: lenet_fashionmnist/grid_search/lr_schedule
  dir:  ../out/${experiment.name}/${now:%Y-%m-%d_%H-%M-%S}
  sub_dir: gamma=${main_scheduler.lr_gamma},step_size=${main_scheduler.lr_step_size}
paths:
  checkpoints: ${experiment.dir}/${experiment.sub_dir}/checkpoints
  tensorboard: ${experiment.dir}/${experiment.sub_dir}/tensorboard
hydra:
  mode: MULTIRUN
  sweep:
    dir: ${experiment.dir}
    subdir: ${experiment.sub_dir}/logs
  sweeper:
    params:
      # Grid search parameters
      main_scheduler.lr_gamma: 0.1, 0.5, 0.9
      main_scheduler.lr_step_size: 10, 20, 30, 40, 50

# Fixed training parameters
defaults:
  - override /main_scheduler: step_lr
reproducibility:
  torch_seed: 89
  shuffle_seed: 858
dataset:
  transform_train:
    crop_scale:
      lower: 0.8  # based on "random_cropping.yaml" grid search
    flip_prob: 0.5  # based on "horizontal_flipping.yaml" grid search
  transform_val:
    resize_size: ${model.input_size}  # matches the input size of the model
dataloader:
  num_workers: 0
  batch_size: 64  # based on "batch_size_lr.yaml" grid search
training:
  num_epochs: 200
optimizer:
  lr: 0.01  # based on "batch_size_lr.yaml" grid search
  momentum: 0.8  # based on "momentum_wd.yaml" grid search
  weight_decay: 1e-3  # based on "momentum_wd.yaml" & "horizontal_flipping.yaml" grid searches
performance:
  patience: 20
