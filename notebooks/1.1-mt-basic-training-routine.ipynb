{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3c3055-2e8a-474e-8fdf-35275cd5eb4c",
   "metadata": {},
   "source": [
    "# Basic Training Routine\n",
    "\n",
    "**NOTE**: No longer up-to-date with source code (i.e., LeNet architecture, loading of FashionMNIST dataset, and training algorithms have changed)!\n",
    "\n",
    "\n",
    "## Imports\n",
    "\n",
    "To start off, we import all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274dcce5-d20b-4986-8eaa-bab4f2fb484f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:26:56.545922Z",
     "start_time": "2024-03-12T14:26:55.154667Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b796f-420a-4f2a-8f94-c9d558dd4e93",
   "metadata": {},
   "source": [
    "## Constants & Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b116a00-3c99-4fce-980c-923ed3bbfc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.132884Z",
     "start_time": "2024-03-11T16:26:51.131301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Timestamp for logging purposes\n",
    "now = datetime.today()\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RUNS_DIR = Path(f\"../logs/tensorboard/{now.strftime('%Y-%m-%d')}/{now.strftime('%H-%M-%S')}\")\n",
    "\n",
    "# Params\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "NUM_EPOCHS = 10\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Training\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Logging\n",
    "EPOCH_INDEX = 0\n",
    "INTRA_EPOCH_UPDATES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50643c89796b50c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, we set up a **TensorBoard writer** to log the training process later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77ac2ef40ac1077",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(RUNS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e98cb-cc0f-4efc-a567-4be047560abb",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We create a transform that transforms the inputs (`PIL.Image.Image`) to `Image` instances (precisely, `torchvision.tv_tensors.Image`),\n",
    "which are largely interchangeable with regular tensors. See [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6909a850-6c33-4dd1-bb1e-44d15cb01fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.135026Z",
     "start_time": "2024-03-11T16:26:51.133435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Effectively the same as the 'ToTensor' transformation in v1, followed by normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),                           # convert to Image\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # scale data to have values in [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))            # normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d2827-08a3-4ba9-8235-8994e0a06839",
   "metadata": {},
   "source": [
    "We create separate datasets for training and validation.\n",
    "- `train=True` creates dataset from `train-images-idx3-ubyte` (60k training images)\n",
    "- `train=False` creates dataset from `t10k-images-idx3-ubyte` (10k test images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb0deb7-5783-44f4-83a5-6a7ba9412fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.150130Z",
     "start_time": "2024-03-11T16:26:51.135989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_set = torchvision.datasets.FashionMNIST(DATA_DIR, train=True, transform=transform, download=True)\n",
    "val_set = torchvision.datasets.FashionMNIST(DATA_DIR, train=False, transform=transform, download=True)\n",
    "\n",
    "# Create dataloaders from datasets, shuffle only during training\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0449e8-a0ff-41cc-8c24-9955dd9b0c50",
   "metadata": {},
   "source": [
    "Next, we manually define the class labels used by the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01eb663-6c9b-4ded-9e87-8c668646b4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.152279Z",
     "start_time": "2024-03-11T16:26:51.150688Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_LABELS = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle Boot\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3824a8-c7c4-4003-afd6-1c12ae65ff98",
   "metadata": {},
   "source": [
    "Finally, we visualize a few images from the validation set using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf54abf-bbbd-4557-a75b-8f842afc7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab sample images and labels from validation set\n",
    "sample_val_imgs, sample_val_labels = next(iter(val_loader))\n",
    "img_grid = torchvision.utils.make_grid(sample_val_imgs)\n",
    "\n",
    "# Write to Tensorboard\n",
    "writer.add_image(\"FashionMNIST Sample Validation Images\", img_grid)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146d10a-47f5-4861-b70f-6a7dd58a1cfc",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "We implement a slight modification of the **LeNet** model proposed by [LeCun et al. (1998)](https://direct.mit.edu/neco/article-abstract/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code?redirectedFrom=fulltext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088c6efc-4778-4457-94bf-2fa30d669ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.155472Z",
     "start_time": "2024-03-11T16:26:51.152828Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"LeNet-5 architecture proposed by LeCun et al. (1998).\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709de4f-e0af-4ab3-a35e-957997c70f09",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "We start with a function that **trains\\validates a model for a single epoch**. We also report progress to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63f869bb-2c71-4785-b0dd-8d850020d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, loss_fn, optimizer=None):\n",
    "    # Initialize dictionary to log intra-epoch results\n",
    "    logs = {\n",
    "        \"global_step\": [],\n",
    "        \"loss\": [],\n",
    "        \"accuracy\": []\n",
    "    }\n",
    "    \n",
    "    # Running totals to report progress to TensorBoard\n",
    "    running_samples = 0\n",
    "    running_loss = 0.\n",
    "    running_correct = 0\n",
    "\n",
    "    # Set training/evaluation mode\n",
    "    is_training = optimizer is not None\n",
    "    model.train(is_training)\n",
    "\n",
    "    # Determine batch indices at which to log to TensorBoard\n",
    "    num_batches = len(dataloader)\n",
    "    log_indices = torch.linspace(\n",
    "        0, num_batches - 1, INTRA_EPOCH_UPDATES + 1\n",
    "    ).int().tolist()\n",
    "    if EPOCH_INDEX != 0:\n",
    "        log_indices = log_indices[1:]\n",
    "\n",
    "    # Set tags for TensorBoard logging\n",
    "    tag_loss = f\"Loss/{'Train' if is_training else 'Val'}\"\n",
    "    tag_accuracy = f\"Accuracy/{'Train' if is_training else 'Val'}\"\n",
    "\n",
    "    # Prepare progress bar\n",
    "    desc = (f\"Epoch [{EPOCH_INDEX + 1:02}/{NUM_EPOCHS}]  \"\n",
    "            f\"{'Train' if is_training else 'Val'}\")\n",
    "    pbar = tqdm(dataloader, desc=desc, leave=False, unit=\"batch\")\n",
    "\n",
    "    # Disable gradients during evaluation\n",
    "    with (torch.set_grad_enabled(is_training)):\n",
    "        for batch_index, (inputs, targets) in enumerate(pbar):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # Keep track of the number of samples\n",
    "            samples = len(targets)\n",
    "            running_samples += samples\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * samples\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            correct = (predictions == targets).sum().item()\n",
    "            running_correct += correct\n",
    "\n",
    "            # Update progress bar\n",
    "            avg_batch_loss = running_loss / running_samples\n",
    "            avg_batch_accuracy = (running_correct / running_samples) * 100  # in pct\n",
    "            pbar.set_postfix(\n",
    "                loss=avg_batch_loss,\n",
    "                accuracy=avg_batch_accuracy\n",
    "            )\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()  # zero gradients\n",
    "                loss.backward()        # compute gradients\n",
    "                optimizer.step()       # update weights\n",
    "\n",
    "            # Log batch loss and accuracy\n",
    "            if batch_index in log_indices:\n",
    "                # Log to TensorBoard\n",
    "                global_step = EPOCH_INDEX * num_batches + batch_index + 1\n",
    "                writer.add_scalar(tag_loss, avg_batch_loss, global_step)\n",
    "                writer.add_scalar(tag_accuracy, avg_batch_accuracy, global_step)\n",
    "\n",
    "                # Log to dictionary\n",
    "                logs[\"global_step\"].append(global_step)\n",
    "                logs[\"loss\"].append(avg_batch_loss)\n",
    "                logs[\"accuracy\"].append(avg_batch_accuracy)\n",
    "\n",
    "                # Reset running totals\n",
    "                running_samples = 0\n",
    "                running_loss = 0.\n",
    "                running_correct = 0\n",
    "\n",
    "    # Flush writer after epoch for live updates\n",
    "    writer.flush()\n",
    "    \n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7f32f-4870-4bfa-90ab-b7f9f01e1c20",
   "metadata": {},
   "source": [
    "To **train our model**, we simply iteratively call the `run_epoch` function with and without passing an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adafd9a3-49ea-4d40-9830-6c2ede98838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer):\n",
    "    global EPOCH_INDEX\n",
    "\n",
    "    # Initialize dictionary to log results\n",
    "    logs = {\n",
    "        \"train\": {\n",
    "            \"global_step\": [],\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"global_step\": [],\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for _ in range(NUM_EPOCHS):\n",
    "        # Train and validate model\n",
    "        train_logs = run_epoch(model, train_loader, loss_fn, optimizer)\n",
    "        val_logs = run_epoch(model, val_loader, loss_fn)\n",
    "\n",
    "        # Log results\n",
    "        logs[\"train\"][\"global_step\"].extend(train_logs[\"global_step\"])\n",
    "        logs[\"train\"][\"loss\"].extend(train_logs[\"loss\"])\n",
    "        logs[\"train\"][\"accuracy\"].extend(train_logs[\"accuracy\"])\n",
    "        logs[\"val\"][\"global_step\"].extend(val_logs[\"global_step\"])\n",
    "        logs[\"val\"][\"loss\"].extend(val_logs[\"loss\"])\n",
    "        logs[\"val\"][\"accuracy\"].extend(val_logs[\"accuracy\"])\n",
    "\n",
    "        # Increment epoch index\n",
    "        EPOCH_INDEX += 1\n",
    "\n",
    "    # Close TensorBoard writer and inform user of training completion\n",
    "    writer.close()\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aeeec-290c-49ed-b911-79cce90d334c",
   "metadata": {},
   "source": [
    "We also need a function to regularly **save checkpoints during training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf8f6e0-a935-4e0f-9bc0-39762f47239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa887110c5af306",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "First, we check the **target device for training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "823c9d69-18e6-4b16-ac63-0d5bb31dfbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46630fac4a40412c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We create an **instance of the LeNet model architecture**, move the network to the target device, and visualize the network's\n",
    "architecture using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca41c782c08c5469",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "network = LeNet().to(DEVICE)\n",
    "\n",
    "# Visualize architecture using TensorBoard\n",
    "writer.add_graph(network, sample_val_imgs.to(DEVICE))\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434715db4802c603",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, we set up our **loss function** and **optimizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c57feb6acebda25b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr=LR, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2c46-b0f9-4aa1-bcd0-239e8494b5e9",
   "metadata": {},
   "source": [
    "We open **TensorBoard** to track the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d424deab-b2dc-4418-8ed6-8286f2f8fb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-84a8f2056708ee00\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-84a8f2056708ee00\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../logs/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb755f3e-a8c4-42c2-ac3e-37c370a15705",
   "metadata": {},
   "source": [
    "Finally, we start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce080c5-5980-4dc9-b6ac-8dd83bdb223b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:28:12.122192Z",
     "start_time": "2024-03-11T16:26:51.191740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "logs = train_model(network, train_loader, val_loader, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
