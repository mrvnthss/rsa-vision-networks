{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3c3055-2e8a-474e-8fdf-35275cd5eb4c",
   "metadata": {},
   "source": [
    "# Proof of Concept\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274dcce5-d20b-4986-8eaa-bab4f2fb484f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.130508Z",
     "start_time": "2024-03-11T16:26:46.991600Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b796f-420a-4f2a-8f94-c9d558dd4e93",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b116a00-3c99-4fce-980c-923ed3bbfc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.132884Z",
     "start_time": "2024-03-11T16:26:51.131301Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DATA_DIR = Path(\"../data/raw\")\n",
    "NUM_EPOCHS = 5\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e98cb-cc0f-4efc-a567-4be047560abb",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ad021-6d82-41e7-952e-67a3ed0c13b4",
   "metadata": {},
   "source": [
    "We create a transform that transforms the inputs (`PIL.Image.Image`) to `Image` instances (precisely, `torchvision.tv_tensors.Image`),\n",
    "which are largely interchangeable with regular tensors. See [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6909a850-6c33-4dd1-bb1e-44d15cb01fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.135026Z",
     "start_time": "2024-03-11T16:26:51.133435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Effectively the same as the 'ToTensor' transformation in v1, followed by normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),                           # convert to Image\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # scale data to have values in [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))            # normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d2827-08a3-4ba9-8235-8994e0a06839",
   "metadata": {},
   "source": [
    "We create separate datasets for training and validation.\n",
    "- `train=True` creates dataset from `train-images-idx3-ubyte` (60k training images)\n",
    "- `train=False` creates dataset from `t10k-images-idx3-ubyte` (10k test images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb0deb7-5783-44f4-83a5-6a7ba9412fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.150130Z",
     "start_time": "2024-03-11T16:26:51.135989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_set = torchvision.datasets.FashionMNIST(DATA_DIR, train=True, transform=transform, download=True)\n",
    "val_set = torchvision.datasets.FashionMNIST(DATA_DIR, train=False, transform=transform, download=True)\n",
    "\n",
    "# Create dataloaders from datasets, shuffle only during training\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0449e8-a0ff-41cc-8c24-9955dd9b0c50",
   "metadata": {},
   "source": [
    "Finally, we manually define the class labels used by the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01eb663-6c9b-4ded-9e87-8c668646b4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.152279Z",
     "start_time": "2024-03-11T16:26:51.150688Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_LABELS = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146d10a-47f5-4861-b70f-6a7dd58a1cfc",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "We implement a slight modification of the **LeNet** model proposed by [LeCun et al. (1998)](https://direct.mit.edu/neco/article-abstract/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code?redirectedFrom=fulltext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088c6efc-4778-4457-94bf-2fa30d669ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.155472Z",
     "start_time": "2024-03-11T16:26:51.152828Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"LeNet-5 architecture proposed by LeCun et al. (1998).\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709de4f-e0af-4ab3-a35e-957997c70f09",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bf11fa-0324-4842-be77-2447e4e4c7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.159420Z",
     "start_time": "2024-03-11T16:26:51.155980Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(network, dataloader, loss_fn, optimizer):\n",
    "    \"\"\"Train a network on a training set for one full epoch.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the last set of 250 batches.\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    avg_batch_loss = 0.\n",
    "\n",
    "    network.train()\n",
    "\n",
    "    for batch_index, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()            # zero gradients\n",
    "        outputs = network(inputs)        # perform forward pass\n",
    "        loss = loss_fn(outputs, labels)  # compute batch loss\n",
    "        loss.backward()                  # compute gradients\n",
    "        optimizer.step()                 # adjust network parameters\n",
    "        \n",
    "        running_loss += loss.item()  # accumulate loss over multiple batches\n",
    "\n",
    "        if (batch_index + 1) % 250 == 0:\n",
    "            avg_batch_loss = running_loss / 250  # avg loss per batch\n",
    "            print(f\"   Batch {batch_index + 1:04}   Loss: {avg_batch_loss:.4f}\")\n",
    "            running_loss = 0.  # reset running loss\n",
    "    \n",
    "    return avg_batch_loss\n",
    "\n",
    "\n",
    "def test_network(network, dataloader, loss_fn):\n",
    "    \"\"\"Test a network on a test set using the provided dataloader and loss function.\n",
    "    \n",
    "    Returns:\n",
    "        float: The average loss over the last set of 250 batches.\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    avg_batch_loss = 0.\n",
    "\n",
    "    network.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # NOTE: Using 'enumerate(dataloader)' let's us track the batch we're\n",
    "        #       currently in for intra-epoch reporting.\n",
    "        for batch_index, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = network(inputs)  # perform forward pass\n",
    "            loss = loss_fn(outputs, labels)  # compute batch loss\n",
    "            \n",
    "            running_loss += loss.item()  # accumulate loss over multiple batches\n",
    "\n",
    "            if (batch_index + 1) % 250 == 0:\n",
    "                avg_batch_loss = running_loss / 250  # avg loss per batch\n",
    "                running_loss = 0.  # reset running loss\n",
    "\n",
    "    return avg_batch_loss\n",
    "\n",
    "\n",
    "def save_checkpoint():\n",
    "    pass\n",
    "\n",
    "\n",
    "def train_network(network, train_loader, val_loader, loss_fn, optimizer, num_epochs, start_epoch=0):\n",
    "    \"\"\"Train and validate a network for multiple epochs at once.\"\"\"\n",
    "    epoch_index = start_epoch\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"EPOCH {epoch_index + 1}\")\n",
    "\n",
    "        # Iteratively train and validate the network\n",
    "        _ = train_one_epoch(network, train_loader, loss_fn, optimizer)\n",
    "        avg_val_loss = test_network(network, val_loader, loss_fn)\n",
    "\n",
    "        # Report results for validation set\n",
    "        print(f\"   Validation   Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        epoch_index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed77093-b124-4ab3-8565-99950c368df4",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "First, we set the **target device for training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded390a4-2685-44c9-b2fc-ab2bb029eeb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.174560Z",
     "start_time": "2024-03-11T16:26:51.159995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea64e7d-1311-43e2-8ea5-0455b4d58e6d",
   "metadata": {},
   "source": [
    "We create an **instance of the LeNet model architecture**, and move the network to the target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ae34b3-a2b4-43ae-b930-d3c2e4758604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.188112Z",
     "start_time": "2024-03-11T16:26:51.175248Z"
    }
   },
   "outputs": [],
   "source": [
    "network = LeNet().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249901f1-a1d8-46df-8980-e5b0f497ce5a",
   "metadata": {},
   "source": [
    "Next, we set up our **optimizer**.  \n",
    "**IMPORTANT**: The optimizer has to be initialized with the network's parameters **after** the model has been moved to the target device for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb34488-ac17-440a-a0f5-db44ac5294a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.191257Z",
     "start_time": "2024-03-11T16:26:51.189708Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(network.parameters(), lr=LR, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb755f3e-a8c4-42c2-ac3e-37c370a15705",
   "metadata": {},
   "source": [
    "Finally, we start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce080c5-5980-4dc9-b6ac-8dd83bdb223b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:28:12.122192Z",
     "start_time": "2024-03-11T16:26:51.191740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "   Batch 0250   Loss: 2.2934\n",
      "   Batch 0500   Loss: 2.2413\n",
      "   Batch 0750   Loss: 1.6177\n",
      "   Batch 1000   Loss: 0.9227\n",
      "   Batch 1250   Loss: 0.8137\n",
      "   Batch 1500   Loss: 0.7336\n",
      "   Batch 1750   Loss: 0.7092\n",
      "   Batch 2000   Loss: 0.6724\n",
      "   Batch 2250   Loss: 0.6819\n",
      "   Batch 2500   Loss: 0.6406\n",
      "   Batch 2750   Loss: 0.6075\n",
      "   Batch 3000   Loss: 0.5852\n",
      "   Batch 3250   Loss: 0.5760\n",
      "   Batch 3500   Loss: 0.5552\n",
      "   Batch 3750   Loss: 0.5789\n",
      "   Validation   Loss: 0.5684\n",
      "EPOCH 2\n",
      "   Batch 0250   Loss: 0.5487\n",
      "   Batch 0500   Loss: 0.5582\n",
      "   Batch 0750   Loss: 0.5320\n",
      "   Batch 1000   Loss: 0.5063\n",
      "   Batch 1250   Loss: 0.4930\n",
      "   Batch 1500   Loss: 0.5061\n",
      "   Batch 1750   Loss: 0.4823\n",
      "   Batch 2000   Loss: 0.5065\n",
      "   Batch 2250   Loss: 0.4883\n",
      "   Batch 2500   Loss: 0.4762\n",
      "   Batch 2750   Loss: 0.4581\n",
      "   Batch 3000   Loss: 0.4775\n",
      "   Batch 3250   Loss: 0.4628\n",
      "   Batch 3500   Loss: 0.4401\n",
      "   Batch 3750   Loss: 0.4477\n",
      "   Validation   Loss: 0.4847\n",
      "EPOCH 3\n",
      "   Batch 0250   Loss: 0.4313\n",
      "   Batch 0500   Loss: 0.4519\n",
      "   Batch 0750   Loss: 0.4332\n",
      "   Batch 1000   Loss: 0.4597\n",
      "   Batch 1250   Loss: 0.4060\n",
      "   Batch 1500   Loss: 0.4058\n",
      "   Batch 1750   Loss: 0.4083\n",
      "   Batch 2000   Loss: 0.3981\n",
      "   Batch 2250   Loss: 0.4075\n",
      "   Batch 2500   Loss: 0.4125\n",
      "   Batch 2750   Loss: 0.3985\n",
      "   Batch 3000   Loss: 0.3817\n",
      "   Batch 3250   Loss: 0.3999\n",
      "   Batch 3500   Loss: 0.3958\n",
      "   Batch 3750   Loss: 0.3838\n",
      "   Validation   Loss: 0.4229\n",
      "EPOCH 4\n",
      "   Batch 0250   Loss: 0.3803\n",
      "   Batch 0500   Loss: 0.3739\n",
      "   Batch 0750   Loss: 0.3725\n",
      "   Batch 1000   Loss: 0.3684\n",
      "   Batch 1250   Loss: 0.3622\n",
      "   Batch 1500   Loss: 0.3842\n",
      "   Batch 1750   Loss: 0.3816\n",
      "   Batch 2000   Loss: 0.3779\n",
      "   Batch 2250   Loss: 0.3700\n",
      "   Batch 2500   Loss: 0.3677\n",
      "   Batch 2750   Loss: 0.3684\n",
      "   Batch 3000   Loss: 0.3468\n",
      "   Batch 3250   Loss: 0.3526\n",
      "   Batch 3500   Loss: 0.3623\n",
      "   Batch 3750   Loss: 0.3491\n",
      "   Validation   Loss: 0.3813\n",
      "EPOCH 5\n",
      "   Batch 0250   Loss: 0.3525\n",
      "   Batch 0500   Loss: 0.3562\n",
      "   Batch 0750   Loss: 0.3673\n",
      "   Batch 1000   Loss: 0.3482\n",
      "   Batch 1250   Loss: 0.3474\n",
      "   Batch 1500   Loss: 0.3424\n",
      "   Batch 1750   Loss: 0.3633\n",
      "   Batch 2000   Loss: 0.3336\n",
      "   Batch 2250   Loss: 0.3309\n",
      "   Batch 2500   Loss: 0.3162\n",
      "   Batch 2750   Loss: 0.3197\n",
      "   Batch 3000   Loss: 0.3124\n",
      "   Batch 3250   Loss: 0.3404\n",
      "   Batch 3500   Loss: 0.3349\n",
      "   Batch 3750   Loss: 0.3217\n",
      "   Validation   Loss: 0.3615\n"
     ]
    }
   ],
   "source": [
    "train_network(network, train_loader, val_loader, LOSS_FN, optimizer, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
