{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3c3055-2e8a-474e-8fdf-35275cd5eb4c",
   "metadata": {},
   "source": [
    "# Proof of Concept\n",
    "## Imports\n",
    "\n",
    "To start off, we import all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274dcce5-d20b-4986-8eaa-bab4f2fb484f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:26:56.545922Z",
     "start_time": "2024-03-12T14:26:55.154667Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b796f-420a-4f2a-8f94-c9d558dd4e93",
   "metadata": {},
   "source": [
    "## Constants & Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b116a00-3c99-4fce-980c-923ed3bbfc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.132884Z",
     "start_time": "2024-03-11T16:26:51.131301Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/raw\")\n",
    "RUNS_DIR = Path(\"../runs/FashionMNIST\")\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 16\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "LR = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "REPORTING_FREQ = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50643c89796b50c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, we set up a **TensorBoard writer** to log the training process later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77ac2ef40ac1077",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(RUNS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e98cb-cc0f-4efc-a567-4be047560abb",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We create a transform that transforms the inputs (`PIL.Image.Image`) to `Image` instances (precisely, `torchvision.tv_tensors.Image`),\n",
    "which are largely interchangeable with regular tensors. See [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6909a850-6c33-4dd1-bb1e-44d15cb01fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.135026Z",
     "start_time": "2024-03-11T16:26:51.133435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Effectively the same as the 'ToTensor' transformation in v1, followed by normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),                           # convert to Image\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # scale data to have values in [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))            # normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d2827-08a3-4ba9-8235-8994e0a06839",
   "metadata": {},
   "source": [
    "We create separate datasets for training and validation.\n",
    "- `train=True` creates dataset from `train-images-idx3-ubyte` (60k training images)\n",
    "- `train=False` creates dataset from `t10k-images-idx3-ubyte` (10k test images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb0deb7-5783-44f4-83a5-6a7ba9412fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.150130Z",
     "start_time": "2024-03-11T16:26:51.135989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_set = torchvision.datasets.FashionMNIST(DATA_DIR, train=True, transform=transform, download=True)\n",
    "val_set = torchvision.datasets.FashionMNIST(DATA_DIR, train=False, transform=transform, download=True)\n",
    "\n",
    "# Create dataloaders from datasets, shuffle only during training\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0449e8-a0ff-41cc-8c24-9955dd9b0c50",
   "metadata": {},
   "source": [
    "Next, we manually define the class labels used by the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01eb663-6c9b-4ded-9e87-8c668646b4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.152279Z",
     "start_time": "2024-03-11T16:26:51.150688Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_LABELS = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3824a8-c7c4-4003-afd6-1c12ae65ff98",
   "metadata": {},
   "source": [
    "Finally, we visualize a few images from the validation set using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf54abf-bbbd-4557-a75b-8f842afc7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab sample images and labels from validation set\n",
    "sample_val_imgs, sample_val_labels = next(iter(val_loader))\n",
    "img_grid = torchvision.utils.make_grid(sample_val_imgs)\n",
    "\n",
    "# Write to Tensorboard\n",
    "writer.add_image(\"FashionMNIST Sample Validation Images\", img_grid)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146d10a-47f5-4861-b70f-6a7dd58a1cfc",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "We implement a slight modification of the **LeNet** model proposed by [LeCun et al. (1998)](https://direct.mit.edu/neco/article-abstract/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code?redirectedFrom=fulltext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088c6efc-4778-4457-94bf-2fa30d669ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.155472Z",
     "start_time": "2024-03-11T16:26:51.152828Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"LeNet-5 architecture proposed by LeCun et al. (1998).\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709de4f-e0af-4ab3-a35e-957997c70f09",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "We start with a function that **trains a network for a single epoch**. We report the training progress to TensorBoard and\n",
    "additionally print to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5bf11fa-0324-4842-be77-2447e4e4c7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:26:51.159420Z",
     "start_time": "2024-03-11T16:26:51.155980Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    network, dataloader, loss_fn, optimizer,\n",
    "    num_epochs, epoch_index):\n",
    "    \"\"\"Train a network on a training set for one full epoch.\"\"\"\n",
    "    # Running totals to report training progress to TensorBoard\n",
    "    running_samples = 0\n",
    "    running_loss = 0.\n",
    "    running_correct = 0\n",
    "\n",
    "    # Enable training mode\n",
    "    network.train()\n",
    "\n",
    "    for batch_index, (inputs, labels) in enumerate(dataloader):\n",
    "        # Move data to target device\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Keep track of number of samples\n",
    "        samples = len(labels)\n",
    "        running_samples += samples\n",
    "\n",
    "        optimizer.zero_grad()            # zero gradients\n",
    "        outputs = network(inputs)        # perform forward pass\n",
    "        loss = loss_fn(outputs, labels)  # compute batch loss\n",
    "        loss.backward()                  # compute gradients\n",
    "        optimizer.step()                 # adjust network parameters\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * samples\n",
    "\n",
    "        # Count correct predictions and add to running total\n",
    "        _, predictions = torch.max(outputs.data, dim=1)\n",
    "        running_correct += (predictions == labels).sum().item()\n",
    "\n",
    "        # Report training progress to TensorBoard\n",
    "        if (batch_index + 1) % REPORTING_FREQ == 0:\n",
    "            # Compute current global step (i.e., across epochs)\n",
    "            global_step = epoch_index * len(dataloader) + batch_index\n",
    "            \n",
    "            # Compute average loss and average accuracy per batch\n",
    "            avg_batch_loss = running_loss / running_samples\n",
    "            avg_batch_accuracy = (running_correct / running_samples) * 100  # in pct\n",
    "\n",
    "            # Write to TensorBoard\n",
    "            writer.add_scalar(\"Training Loss\", avg_batch_loss, global_step)\n",
    "            writer.add_scalar(\"Training Accuracy\", avg_batch_accuracy, global_step)\n",
    "\n",
    "            # Print results to console\n",
    "            print(f\"Epoch [{epoch_index + 1:02}/{num_epochs}]   \"\n",
    "                  f\"Batch [{batch_index + 1:03}/{len(dataloader)}]   \"\n",
    "                  f\"Loss: {avg_batch_loss:.4f}   \"\n",
    "                  f\"Acc: {avg_batch_accuracy:02.2f}\")\n",
    "\n",
    "            # Reset running totals\n",
    "            running_samples = 0\n",
    "            running_loss = 0.\n",
    "            running_correct = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64414787-e8cc-4a98-8ab3-75e5f159172a",
   "metadata": {},
   "source": [
    "Next, we implement a function that lets us **evaluate a network on a validation set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2cc45e4-d57b-4ea2-b320-7084d4fbd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_network(\n",
    "    network, dataloader, loss_fn,\n",
    "    num_epochs, epoch_index):\n",
    "    \"\"\"Evaluate a network on a test set using the provided dataloader and loss function.\"\"\"\n",
    "    # Running totals to report training progress to TensorBoard\n",
    "    running_samples = 0\n",
    "    running_loss = 0.\n",
    "    running_correct = 0\n",
    "\n",
    "    # Total values to compute loss and accuracy over entire test set\n",
    "    total_samples = 0\n",
    "    total_loss = 0.\n",
    "    total_correct = 0\n",
    "\n",
    "    # Enable evaluation mode\n",
    "    network.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, labels) in enumerate(dataloader):\n",
    "            # Move data to target device\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            samples = len(labels)\n",
    "            running_samples += samples\n",
    "            total_samples += samples\n",
    "\n",
    "            outputs = network(inputs)  # perform forward pass\n",
    "            loss = loss_fn(outputs, labels)  # compute batch loss\n",
    "\n",
    "             # Accumulate loss\n",
    "            running_loss += loss.item() * samples\n",
    "            total_loss += loss.item() * samples\n",
    "\n",
    "            # Count correct predictions and add to running total(s)\n",
    "            _, predictions = torch.max(outputs.data, dim=1)\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            running_correct += correct\n",
    "            total_correct += correct\n",
    "\n",
    "            # Report results to TensorBoard\n",
    "            if (batch_index + 1) % REPORTING_FREQ == 0:\n",
    "                # Compute current global step (i.e., across epochs)\n",
    "                global_step = epoch_index * len(dataloader) + batch_index\n",
    "                \n",
    "                # Compute average loss and average accuracy per batch\n",
    "                avg_batch_loss = running_loss / running_samples\n",
    "                avg_batch_accuracy = (running_correct / running_samples) * 100  # in pct\n",
    "    \n",
    "                # Write to TensorBoard\n",
    "                writer.add_scalar(\"Validation Loss\", avg_batch_loss, global_step)\n",
    "                writer.add_scalar(\"Validation Accuracy\", avg_batch_accuracy, global_step)\n",
    "    \n",
    "                # Reset running totals\n",
    "                running_samples = 0\n",
    "                running_loss = 0.\n",
    "                running_correct = 0\n",
    "\n",
    "    # Compute loss and accuracy over entire test set\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_accuracy = (total_correct / total_samples) * 100  # in pct\n",
    "\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aeeec-290c-49ed-b911-79cce90d334c",
   "metadata": {},
   "source": [
    "We also need a function to regularly **save checkpoints during training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf8f6e0-a935-4e0f-9bc0-39762f47239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abacb14-0730-4f30-b4f4-8ea2c0f2d181",
   "metadata": {},
   "source": [
    "Finally, we put everything together into a single function that can be used to **train a network for multiple epochs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229ab22e-1359-43ea-a0df-4922971ebd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(\n",
    "        network, train_loader, val_loader, loss_fn, optimizer,\n",
    "        num_epochs, start_epoch=0):\n",
    "    \"\"\"Train and validate a network for multiple epochs at once.\"\"\"\n",
    "    epoch_index = start_epoch\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Iteratively train and validate the network\n",
    "        print(\"Training...\")\n",
    "        train_one_epoch(network, train_loader, loss_fn, optimizer, num_epochs, epoch_index)\n",
    "        print(\"Validating...\")\n",
    "        avg_val_loss, avg_val_accuracy = eval_network(network, val_loader, loss_fn, num_epochs, epoch_index)\n",
    "\n",
    "        # Report results for validation set\n",
    "        print(f\"Epoch [{epoch_index + 1:02}/{num_epochs}]   \"\n",
    "                  \"Validation        \"\n",
    "                  f\"Loss: {avg_val_loss:.4f}   \"\n",
    "                  f\"Acc: {avg_val_accuracy:02.2f}\\n\")\n",
    "\n",
    "        epoch_index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa887110c5af306",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training\n",
    "First, we check the **target device for training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823c9d69-18e6-4b16-ac63-0d5bb31dfbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46630fac4a40412c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We create an **instance of the LeNet model architecture**, move the network to the target device, and visualize the network's\n",
    "architecture using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca41c782c08c5469",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "network = LeNet().to(DEVICE)\n",
    "\n",
    "# Visualize architecture using TensorBoard\n",
    "writer.add_graph(network, sample_val_imgs.to(DEVICE))\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434715db4802c603",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, we set up our **optimizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c57feb6acebda25b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(network.parameters(), lr=LR, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb755f3e-a8c4-42c2-ac3e-37c370a15705",
   "metadata": {},
   "source": [
    "Finally, we start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce080c5-5980-4dc9-b6ac-8dd83bdb223b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T16:28:12.122192Z",
     "start_time": "2024-03-11T16:26:51.191740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch [01/10]   Batch [100/938]   Loss: 2.3031   Acc: 12.58\n",
      "Epoch [01/10]   Batch [200/938]   Loss: 2.2990   Acc: 18.95\n",
      "Epoch [01/10]   Batch [300/938]   Loss: 2.2942   Acc: 27.28\n",
      "Epoch [01/10]   Batch [400/938]   Loss: 2.2855   Acc: 26.02\n",
      "Epoch [01/10]   Batch [500/938]   Loss: 2.2689   Acc: 25.30\n",
      "Epoch [01/10]   Batch [600/938]   Loss: 2.2190   Acc: 28.64\n",
      "Epoch [01/10]   Batch [700/938]   Loss: 2.0040   Acc: 34.23\n",
      "Epoch [01/10]   Batch [800/938]   Loss: 1.4393   Acc: 53.36\n",
      "Epoch [01/10]   Batch [900/938]   Loss: 1.0289   Acc: 62.89\n",
      "Validating...\n",
      "Epoch [01/10]   Validation        Loss: 0.9140   Acc: 66.59\n",
      "\n",
      "Training...\n",
      "Epoch [02/10]   Batch [100/938]   Loss: 0.8472   Acc: 69.48\n",
      "Epoch [02/10]   Batch [200/938]   Loss: 0.8132   Acc: 70.56\n",
      "Epoch [02/10]   Batch [300/938]   Loss: 0.7862   Acc: 70.45\n",
      "Epoch [02/10]   Batch [400/938]   Loss: 0.7630   Acc: 72.00\n",
      "Epoch [02/10]   Batch [500/938]   Loss: 0.7269   Acc: 72.80\n",
      "Epoch [02/10]   Batch [600/938]   Loss: 0.7060   Acc: 73.41\n",
      "Epoch [02/10]   Batch [700/938]   Loss: 0.7244   Acc: 73.38\n",
      "Epoch [02/10]   Batch [800/938]   Loss: 0.6997   Acc: 73.69\n",
      "Epoch [02/10]   Batch [900/938]   Loss: 0.6696   Acc: 75.41\n",
      "Validating...\n",
      "Epoch [02/10]   Validation        Loss: 0.7364   Acc: 72.00\n",
      "\n",
      "Training...\n",
      "Epoch [03/10]   Batch [100/938]   Loss: 0.6749   Acc: 74.47\n",
      "Epoch [03/10]   Batch [200/938]   Loss: 0.6372   Acc: 76.00\n",
      "Epoch [03/10]   Batch [300/938]   Loss: 0.6362   Acc: 75.89\n",
      "Epoch [03/10]   Batch [400/938]   Loss: 0.6127   Acc: 76.94\n",
      "Epoch [03/10]   Batch [500/938]   Loss: 0.6174   Acc: 77.09\n",
      "Epoch [03/10]   Batch [600/938]   Loss: 0.6153   Acc: 76.70\n",
      "Epoch [03/10]   Batch [700/938]   Loss: 0.6208   Acc: 76.69\n",
      "Epoch [03/10]   Batch [800/938]   Loss: 0.6178   Acc: 76.81\n",
      "Epoch [03/10]   Batch [900/938]   Loss: 0.5958   Acc: 77.36\n",
      "Validating...\n",
      "Epoch [03/10]   Validation        Loss: 0.6058   Acc: 77.57\n",
      "\n",
      "Training...\n",
      "Epoch [04/10]   Batch [100/938]   Loss: 0.5730   Acc: 78.22\n",
      "Epoch [04/10]   Batch [200/938]   Loss: 0.5731   Acc: 77.98\n",
      "Epoch [04/10]   Batch [300/938]   Loss: 0.5671   Acc: 78.89\n",
      "Epoch [04/10]   Batch [400/938]   Loss: 0.5701   Acc: 78.53\n",
      "Epoch [04/10]   Batch [500/938]   Loss: 0.5695   Acc: 78.38\n",
      "Epoch [04/10]   Batch [600/938]   Loss: 0.5498   Acc: 79.39\n",
      "Epoch [04/10]   Batch [700/938]   Loss: 0.5436   Acc: 79.56\n",
      "Epoch [04/10]   Batch [800/938]   Loss: 0.5478   Acc: 79.53\n",
      "Epoch [04/10]   Batch [900/938]   Loss: 0.5392   Acc: 79.86\n",
      "Validating...\n",
      "Epoch [04/10]   Validation        Loss: 0.5821   Acc: 78.35\n",
      "\n",
      "Training...\n",
      "Epoch [05/10]   Batch [100/938]   Loss: 0.5378   Acc: 79.58\n",
      "Epoch [05/10]   Batch [200/938]   Loss: 0.5477   Acc: 79.56\n",
      "Epoch [05/10]   Batch [300/938]   Loss: 0.5130   Acc: 80.95\n",
      "Epoch [05/10]   Batch [400/938]   Loss: 0.5014   Acc: 81.16\n",
      "Epoch [05/10]   Batch [500/938]   Loss: 0.5278   Acc: 79.97\n",
      "Epoch [05/10]   Batch [600/938]   Loss: 0.5000   Acc: 81.50\n",
      "Epoch [05/10]   Batch [700/938]   Loss: 0.4950   Acc: 81.38\n",
      "Epoch [05/10]   Batch [800/938]   Loss: 0.4977   Acc: 81.97\n",
      "Epoch [05/10]   Batch [900/938]   Loss: 0.5006   Acc: 81.48\n",
      "Validating...\n",
      "Epoch [05/10]   Validation        Loss: 0.5519   Acc: 79.63\n",
      "\n",
      "Training...\n",
      "Epoch [06/10]   Batch [100/938]   Loss: 0.4991   Acc: 81.27\n",
      "Epoch [06/10]   Batch [200/938]   Loss: 0.4862   Acc: 82.00\n",
      "Epoch [06/10]   Batch [300/938]   Loss: 0.4762   Acc: 81.98\n",
      "Epoch [06/10]   Batch [400/938]   Loss: 0.5133   Acc: 81.02\n",
      "Epoch [06/10]   Batch [500/938]   Loss: 0.4706   Acc: 82.83\n",
      "Epoch [06/10]   Batch [600/938]   Loss: 0.4653   Acc: 82.67\n",
      "Epoch [06/10]   Batch [700/938]   Loss: 0.4728   Acc: 82.14\n",
      "Epoch [06/10]   Batch [800/938]   Loss: 0.4705   Acc: 82.52\n",
      "Epoch [06/10]   Batch [900/938]   Loss: 0.4539   Acc: 82.95\n",
      "Validating...\n",
      "Epoch [06/10]   Validation        Loss: 0.4947   Acc: 81.11\n",
      "\n",
      "Training...\n",
      "Epoch [07/10]   Batch [100/938]   Loss: 0.4559   Acc: 83.02\n",
      "Epoch [07/10]   Batch [200/938]   Loss: 0.4627   Acc: 82.70\n",
      "Epoch [07/10]   Batch [300/938]   Loss: 0.4585   Acc: 82.80\n",
      "Epoch [07/10]   Batch [400/938]   Loss: 0.4503   Acc: 83.08\n",
      "Epoch [07/10]   Batch [500/938]   Loss: 0.4481   Acc: 83.42\n",
      "Epoch [07/10]   Batch [600/938]   Loss: 0.4463   Acc: 83.94\n",
      "Epoch [07/10]   Batch [700/938]   Loss: 0.4644   Acc: 82.91\n",
      "Epoch [07/10]   Batch [800/938]   Loss: 0.4330   Acc: 84.33\n",
      "Epoch [07/10]   Batch [900/938]   Loss: 0.4452   Acc: 83.48\n",
      "Validating...\n",
      "Epoch [07/10]   Validation        Loss: 0.4708   Acc: 82.48\n",
      "\n",
      "Training...\n",
      "Epoch [08/10]   Batch [100/938]   Loss: 0.4369   Acc: 84.25\n",
      "Epoch [08/10]   Batch [200/938]   Loss: 0.4438   Acc: 83.55\n",
      "Epoch [08/10]   Batch [300/938]   Loss: 0.4153   Acc: 85.58\n",
      "Epoch [08/10]   Batch [400/938]   Loss: 0.4284   Acc: 84.39\n",
      "Epoch [08/10]   Batch [500/938]   Loss: 0.4232   Acc: 84.23\n",
      "Epoch [08/10]   Batch [600/938]   Loss: 0.4453   Acc: 83.19\n",
      "Epoch [08/10]   Batch [700/938]   Loss: 0.4372   Acc: 84.12\n",
      "Epoch [08/10]   Batch [800/938]   Loss: 0.4249   Acc: 84.59\n",
      "Epoch [08/10]   Batch [900/938]   Loss: 0.4255   Acc: 84.50\n",
      "Validating...\n",
      "Epoch [08/10]   Validation        Loss: 0.4433   Acc: 83.68\n",
      "\n",
      "Training...\n",
      "Epoch [09/10]   Batch [100/938]   Loss: 0.4187   Acc: 84.39\n",
      "Epoch [09/10]   Batch [200/938]   Loss: 0.4177   Acc: 84.44\n",
      "Epoch [09/10]   Batch [300/938]   Loss: 0.4183   Acc: 84.77\n",
      "Epoch [09/10]   Batch [400/938]   Loss: 0.4139   Acc: 85.03\n",
      "Epoch [09/10]   Batch [500/938]   Loss: 0.4049   Acc: 85.30\n",
      "Epoch [09/10]   Batch [600/938]   Loss: 0.4309   Acc: 84.38\n",
      "Epoch [09/10]   Batch [700/938]   Loss: 0.4002   Acc: 85.31\n",
      "Epoch [09/10]   Batch [800/938]   Loss: 0.3924   Acc: 85.28\n",
      "Epoch [09/10]   Batch [900/938]   Loss: 0.3908   Acc: 85.52\n",
      "Validating...\n",
      "Epoch [09/10]   Validation        Loss: 0.4246   Acc: 84.30\n",
      "\n",
      "Training...\n",
      "Epoch [10/10]   Batch [100/938]   Loss: 0.3901   Acc: 85.55\n",
      "Epoch [10/10]   Batch [200/938]   Loss: 0.4104   Acc: 84.81\n",
      "Epoch [10/10]   Batch [300/938]   Loss: 0.3821   Acc: 85.73\n",
      "Epoch [10/10]   Batch [400/938]   Loss: 0.3970   Acc: 85.72\n",
      "Epoch [10/10]   Batch [500/938]   Loss: 0.4049   Acc: 85.72\n",
      "Epoch [10/10]   Batch [600/938]   Loss: 0.3825   Acc: 86.09\n",
      "Epoch [10/10]   Batch [700/938]   Loss: 0.3981   Acc: 85.50\n",
      "Epoch [10/10]   Batch [800/938]   Loss: 0.3821   Acc: 86.34\n",
      "Epoch [10/10]   Batch [900/938]   Loss: 0.3944   Acc: 85.86\n",
      "Validating...\n",
      "Epoch [10/10]   Validation        Loss: 0.4091   Acc: 85.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network(network, train_loader, val_loader, LOSS_FN, optimizer, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
