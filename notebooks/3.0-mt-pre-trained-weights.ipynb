{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e17b36d-5c59-4b7b-a5f8-46eadc943bd9",
   "metadata": {},
   "source": [
    "# Using Pre-Trained Weights\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html  \n",
    "https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights  \n",
    "https://pytorch.org/vision/stable/_modules/torchvision/models/_api.html  \n",
    "https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b331cb5-908e-4ad8-9eef-37a8d1f881d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.848677Z",
     "start_time": "2024-05-14T09:00:23.125757Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d26ea6-9460-483a-8bcb-f35cedca0a8e",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "https://pytorch.org/vision/stable/generated/torchvision.models.list_models.html\n",
    "\n",
    "The `list_models()` function can be used to list all available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a23ab8-879a-4bda-81a1-aedbfbc28882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.851323Z",
     "start_time": "2024-05-14T09:00:26.849508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "convnext_base\n",
      "convnext_large\n",
      "convnext_small\n",
      "convnext_tiny\n",
      "deeplabv3_mobilenet_v3_large\n",
      "deeplabv3_resnet101\n",
      "deeplabv3_resnet50\n",
      "densenet121\n",
      "densenet161\n",
      "densenet169\n",
      "densenet201\n",
      "efficientnet_b0\n",
      "efficientnet_b1\n",
      "efficientnet_b2\n",
      "efficientnet_b3\n",
      "efficientnet_b4\n",
      "efficientnet_b5\n",
      "efficientnet_b6\n",
      "efficientnet_b7\n",
      "efficientnet_v2_l\n",
      "efficientnet_v2_m\n",
      "efficientnet_v2_s\n",
      "fasterrcnn_mobilenet_v3_large_320_fpn\n",
      "fasterrcnn_mobilenet_v3_large_fpn\n",
      "fasterrcnn_resnet50_fpn\n",
      "fasterrcnn_resnet50_fpn_v2\n",
      "fcn_resnet101\n",
      "fcn_resnet50\n",
      "fcos_resnet50_fpn\n",
      "googlenet\n",
      "inception_v3\n",
      "keypointrcnn_resnet50_fpn\n",
      "lraspp_mobilenet_v3_large\n",
      "maskrcnn_resnet50_fpn\n",
      "maskrcnn_resnet50_fpn_v2\n",
      "maxvit_t\n",
      "mc3_18\n",
      "mnasnet0_5\n",
      "mnasnet0_75\n",
      "mnasnet1_0\n",
      "mnasnet1_3\n",
      "mobilenet_v2\n",
      "mobilenet_v3_large\n",
      "mobilenet_v3_small\n",
      "mvit_v1_b\n",
      "mvit_v2_s\n",
      "quantized_googlenet\n",
      "quantized_inception_v3\n",
      "quantized_mobilenet_v2\n",
      "quantized_mobilenet_v3_large\n",
      "quantized_resnet18\n",
      "quantized_resnet50\n",
      "quantized_resnext101_32x8d\n",
      "quantized_resnext101_64x4d\n",
      "quantized_shufflenet_v2_x0_5\n",
      "quantized_shufflenet_v2_x1_0\n",
      "quantized_shufflenet_v2_x1_5\n",
      "quantized_shufflenet_v2_x2_0\n",
      "r2plus1d_18\n",
      "r3d_18\n",
      "raft_large\n",
      "raft_small\n",
      "regnet_x_16gf\n",
      "regnet_x_1_6gf\n",
      "regnet_x_32gf\n",
      "regnet_x_3_2gf\n",
      "regnet_x_400mf\n",
      "regnet_x_800mf\n",
      "regnet_x_8gf\n",
      "regnet_y_128gf\n",
      "regnet_y_16gf\n",
      "regnet_y_1_6gf\n",
      "regnet_y_32gf\n",
      "regnet_y_3_2gf\n",
      "regnet_y_400mf\n",
      "regnet_y_800mf\n",
      "regnet_y_8gf\n",
      "resnet101\n",
      "resnet152\n",
      "resnet18\n",
      "resnet34\n",
      "resnet50\n",
      "resnext101_32x8d\n",
      "resnext101_64x4d\n",
      "resnext50_32x4d\n",
      "retinanet_resnet50_fpn\n",
      "retinanet_resnet50_fpn_v2\n",
      "s3d\n",
      "shufflenet_v2_x0_5\n",
      "shufflenet_v2_x1_0\n",
      "shufflenet_v2_x1_5\n",
      "shufflenet_v2_x2_0\n",
      "squeezenet1_0\n",
      "squeezenet1_1\n",
      "ssd300_vgg16\n",
      "ssdlite320_mobilenet_v3_large\n",
      "swin3d_b\n",
      "swin3d_s\n",
      "swin3d_t\n",
      "swin_b\n",
      "swin_s\n",
      "swin_t\n",
      "swin_v2_b\n",
      "swin_v2_s\n",
      "swin_v2_t\n",
      "vgg11\n",
      "vgg11_bn\n",
      "vgg13\n",
      "vgg13_bn\n",
      "vgg16\n",
      "vgg16_bn\n",
      "vgg19\n",
      "vgg19_bn\n",
      "vit_b_16\n",
      "vit_b_32\n",
      "vit_h_14\n",
      "vit_l_16\n",
      "vit_l_32\n",
      "wide_resnet101_2\n",
      "wide_resnet50_2\n"
     ]
    }
   ],
   "source": [
    "for model in models.list_models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0cfd35-ae65-4557-960c-a883653f9e66",
   "metadata": {},
   "source": [
    "**Filters** can be used to narrow down the list of available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0771d42-dd83-43f4-af0e-b082811eca3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.853827Z",
     "start_time": "2024-05-14T09:00:26.851952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL VGG MODELS:\n",
      "vgg11\n",
      "vgg11_bn\n",
      "vgg13\n",
      "vgg13_bn\n",
      "vgg16\n",
      "vgg16_bn\n",
      "vgg19\n",
      "vgg19_bn\n",
      "\n",
      "VGG MODELS WITHOUT BATCH NORMALIZATION:\n",
      "vgg11\n",
      "vgg13\n",
      "vgg16\n",
      "vgg19\n"
     ]
    }
   ],
   "source": [
    "# VGG models\n",
    "vgg_models = models.list_models(include=\"vgg*\")\n",
    "print(\"ALL VGG MODELS:\")\n",
    "for model in vgg_models:\n",
    "    print(model)\n",
    "\n",
    "# VGG models that do not use batch normalization\n",
    "vgg_models = models.list_models(include=\"vgg*\", exclude=\"*bn\")\n",
    "print(\"\\nVGG MODELS WITHOUT BATCH NORMALIZATION:\")\n",
    "for model in vgg_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fcfd8-f4da-4aed-8561-ad89487301b8",
   "metadata": {},
   "source": [
    "## Pre-Trained Weights\n",
    "\n",
    "* Available classification weights: https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights\n",
    "* `get_model_weights()` function: https://pytorch.org/vision/stable/generated/torchvision.models.get_model_weights.html\n",
    "* `get_weight()` function: https://pytorch.org/vision/stable/generated/torchvision.models.get_weight.html\n",
    "\n",
    "\n",
    "\n",
    "### Technical Details\n",
    "\n",
    "The `get_model_weights()` function can be used to obtain all available weights. The function has return type `Type[WeightsEnum]`, i.e., it returns the weights enum **class** of the associated model (not an instance of that class). The `WeightsEnum` class defined [here](https://pytorch.org/vision/main/_modules/torchvision/models/_api.html) inherits from Pyton's built-in `Enum` base class for creating enumerated constants, see [here](https://docs.python.org/3/library/enum.html) and [here](https://docs.python.org/3/howto/enum.html#enum-basic-tutorial) for details. This class represents the different pre-trained weights that are available for the given model. Each member of this enumeration is a unique instance of this class, representing a specific set of pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e102608c-d94d-4621-9c78-e0a023579aa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.856464Z",
     "start_time": "2024-05-14T09:00:26.854775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enum 'ResNet50_Weights'>\n"
     ]
    }
   ],
   "source": [
    "weights_enum = models.get_model_weights(\"resnet50\")\n",
    "print(weights_enum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249c283-1930-4410-ad8e-9bd135d138af",
   "metadata": {},
   "source": [
    "We can list all available weights for a given model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fed7071-576a-494c-8934-a840a8aba545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.858448Z",
     "start_time": "2024-05-14T09:00:26.856942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE RESNET-50 WEIGHTS:\n",
      "IMAGENET1K_V1: ResNet50_Weights.IMAGENET1K_V1\n",
      "IMAGENET1K_V2: ResNet50_Weights.IMAGENET1K_V2\n"
     ]
    }
   ],
   "source": [
    "print(\"AVAILABLE RESNET-50 WEIGHTS:\")\n",
    "for weights in weights_enum:\n",
    "    print(f\"{weights.name}: {weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74b3bd-5105-41ac-9171-b8ff0af31633",
   "metadata": {},
   "source": [
    "The `weights_enum` is an enumeration with as many members as there are available pre-trained weights for a given model. Each **member** is technically an **attribute** of `weights_enum`, allowing us to access them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a046f0-aa85-40a1-b378-efc27c3ea0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.860516Z",
     "start_time": "2024-05-14T09:00:26.859008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50_Weights.IMAGENET1K_V1\n",
      "ResNet50_Weights.IMAGENET1K_V2\n"
     ]
    }
   ],
   "source": [
    "# ImageNet weights (old version)\n",
    "print(weights_enum.IMAGENET1K_V1)\n",
    "\n",
    "# ImageNet weights (new version)\n",
    "print(weights_enum.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088ff9f-77d0-4f66-bcf9-19451d45645e",
   "metadata": {},
   "source": [
    "We can also access the individual members as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596ca5d3-12af-4d0e-93df-97f9b99a7357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.862391Z",
     "start_time": "2024-05-14T09:00:26.861029Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet50_weights_v2 = weights_enum[\"IMAGENET1K_V2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059f234-a704-4114-91f9-a43a4bb0bdb9",
   "metadata": {},
   "source": [
    "Each member has a `name` and a `value` associated with it. The `name` of a member is what we just used to access the member in the previous line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2b12ca-7337-4976-a55a-94f1d081eb17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.864253Z",
     "start_time": "2024-05-14T09:00:26.862798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(resnet50_weights_v2.name == \"IMAGENET1K_V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d67858-6613-445a-9626-99314d9afd60",
   "metadata": {},
   "source": [
    "As stated earlier, `weights_enum` is the `WeightsEnum` **class** associated with a given model, and its members/attributes are **instances** of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e92dd34-6706-4f51-b866-d5ade40b7ce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.867435Z",
     "start_time": "2024-05-14T09:00:26.864737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enum 'ResNet50_Weights'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(resnet50_weights_v2))\n",
    "isinstance(resnet50_weights_v2, weights_enum)  # members are instances of the WeightsEnum class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e781440-f45e-45c8-995e-a6c96a62e4a1",
   "metadata": {},
   "source": [
    "Finally, the **value** of each member is an instance of the `Weights` class defined in `torchvision.models._api`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "361e7a0e-c661-4520-89e4-0e521bf71a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.870591Z",
     "start_time": "2024-05-14T09:00:26.868943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.models._api.Weights"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resnet50_weights_v2.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b82fe-99c2-48fb-a916-2512d1e7ec4b",
   "metadata": {},
   "source": [
    "**To sum up**:\n",
    "\n",
    "* The `get_model_weights()` function returns the weights enum class associated with the given model. This `WeightsEnum` class inherits from `enum.Enum`, and is an **enumeration** of all the pre-trained weights available for a model.\n",
    "* The **attributes** of that class are **enumeration members**, and are functionally constants.\n",
    "* Each member has a **name** and **value** associated with it.\n",
    "* The **value** of each member inherits from the `Weights` class defined in `torchvision.models._api`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1bb071-8029-48a2-aa71-f52377a2dec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.872589Z",
     "start_time": "2024-05-14T09:00:26.871045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models._api.Weights'>\n"
     ]
    }
   ],
   "source": [
    "weights_enum = models.get_model_weights(\"resnet50\")  # weights enum class associated with ResNet-50 (enumeration)\n",
    "resnet50_weights_v2 = weights_enum.IMAGENET1K_V2     # member of the enumeration\n",
    "print(type(resnet50_weights_v2.value))               # the actual weights (instance of class `Weights`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bec592-ae01-4630-b832-409e793ec687",
   "metadata": {},
   "source": [
    "Finally, it is also possible to directly access a **particular instance** of the weights enum class of a given model using the `get_weight()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190e5412-4159-4529-a3f0-7fb539091feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:00:26.875084Z",
     "start_time": "2024-05-14T09:00:26.873069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.get_model_weights(\"resnet50\")[\"IMAGENET1K_V2\"] == models.get_weight(\"ResNet50_Weights.IMAGENET1K_V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d21b75-ebbe-4e2a-912c-7532a49a7304",
   "metadata": {},
   "source": [
    "All available pre-trained weights are listed [here](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights).\n",
    "\n",
    "\n",
    "### Working with Pre-Trained Weights\n",
    "\n",
    "Each set of pre-trained weights is an instance of the `Weights` class introduced in `torchvision.models._api` (see [here](https://pytorch.org/vision/main/_modules/torchvision/models/_api.html)). As such, the following useful properties and methods are available:\n",
    "\n",
    "* `.url`: Returns the **url** from which the pre-trained weights can be downloaded.\n",
    "* `.meta`: Returns a `Dict[str, Any]` containing useful metadata about the pre-trained weights, such as **categories** (of the classification task), the **number of parameters**, and the **training recipe**.\n",
    "* `.transforms`: Returns the **preprocessing transforms** to be used when working with the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76025c21-3c10-4231-8728-3d318df02063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL:\n",
      "https://download.pytorch.org/models/vgg11-8a719046.pth\n",
      "\n",
      "KEYS IN META DICT:\n",
      "min_size\n",
      "categories\n",
      "recipe\n",
      "_docs\n",
      "num_params\n",
      "_metrics\n",
      "_ops\n",
      "_file_size\n",
      "\n",
      "FILE SIZE:\n",
      "506.84 MB\n",
      "\n",
      "TRAINING RECIPE:\n",
      "https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\n",
      "\n",
      "PREPROCESSING TRANSFORMS:\n",
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get weights enum for VGG11\n",
    "vgg11_weights = models.get_weight(\"VGG11_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "# URL to download weights\n",
    "print(f\"URL:\\n{vgg11_weights.url}\\n\")\n",
    "\n",
    "# Keys of the dictionary returned by `.dict`\n",
    "print(\"KEYS IN META DICT:\")\n",
    "for k in vgg11_weights.meta:\n",
    "    print(k)\n",
    "\n",
    "# File size of model weights\n",
    "print(f\"\\nFILE SIZE:\\n{vgg11_weights.meta['_file_size']} MB\")\n",
    "\n",
    "# Link to training recipe extracted from dict returned by `.meta`\n",
    "print(f\"\\nTRAINING RECIPE:\\n{vgg11_weights.meta['recipe']}\")\n",
    "\n",
    "# Preprocessing transforms\n",
    "print(f\"\\nPREPROCESSING TRANSFORMS:\\n{vgg11_weights.transforms()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228dfbaa-9fd2-4438-8781-9357fd21c675",
   "metadata": {},
   "source": [
    "**NOTE**: The **_V2** weights improve upon the results of the original paper by using TorchVision’s [new training recipe](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/).\n",
    "\n",
    "To obtain the checkpoint storing the pre-trained weights, we can use the `get_state_dict()` method. This **downloads** the checkpoint and **loads** the state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c8175ae-4382-46fc-8b55-603b47996194",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_weights_dict = vgg11_weights.get_state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac98fa3-1fcb-4c41-97b2-c1c958151dca",
   "metadata": {},
   "source": [
    "Let's take a look at the keys of the ordered dict `vgg11_weights_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc31754-c952-4d43-903c-45c741bf6d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.3.weight\n",
      "features.3.bias\n",
      "features.6.weight\n",
      "features.6.bias\n",
      "features.8.weight\n",
      "features.8.bias\n",
      "features.11.weight\n",
      "features.11.bias\n",
      "features.13.weight\n",
      "features.13.bias\n",
      "features.16.weight\n",
      "features.16.bias\n",
      "features.18.weight\n",
      "features.18.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n",
      "classifier.6.weight\n",
      "classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for k in vgg11_weights_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e52fe2-0c2a-4ee7-9e95-a10651112a9b",
   "metadata": {},
   "source": [
    "To illustrate how to use these weights, let's implement VGG11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875de0ef-0a92-44f5-9323-2dead0858de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg11 = models.vgg11()\n",
    "print(vgg11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0a05a-fc8d-429a-b457-9a9b42200f81",
   "metadata": {},
   "source": [
    "As we can see, each entry in the `vgg11_weights_dict` corresponds to a layer of VGG11 with trainable parameters.  Also, we can retreive **only** those **layers with trainable parameters** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bd4f8ab-f11c-4b28-a791-9bc128e72ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.3.weight\n",
      "features.3.bias\n",
      "features.6.weight\n",
      "features.6.bias\n",
      "features.8.weight\n",
      "features.8.bias\n",
      "features.11.weight\n",
      "features.11.bias\n",
      "features.13.weight\n",
      "features.13.bias\n",
      "features.16.weight\n",
      "features.16.bias\n",
      "features.18.weight\n",
      "features.18.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n",
      "classifier.6.weight\n",
      "classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in vgg11.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bbd737-db33-4078-bd67-83c8b62f73dc",
   "metadata": {},
   "source": [
    "Since we didn't specify any weights when initializing the network, the weights were initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187674fa-4464-4047-8629-e4929a9a372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape identical: True\n",
      "Weights identical: False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Shape identical: \"\n",
    "    f\"{vgg11.state_dict()['features.0.weight'].size() == vgg11_weights_dict['features.0.weight'].size()}\"\n",
    ")\n",
    "print(\n",
    "    \"Weights identical: \"\n",
    "    f\"{torch.all(vgg11.state_dict()['features.0.weight'] == vgg11_weights_dict['features.0.weight'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8e356-dbde-4973-83b9-d75b1d5249b8",
   "metadata": {},
   "source": [
    "Assigning pre-trained weights to **individual layers** is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e7d110c-86d5-4254-88d1-f67e712c6627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights identical: True\n"
     ]
    }
   ],
   "source": [
    "# Assign weights to first convolutional layer\n",
    "vgg11.features[0].weight.data = vgg11_weights_dict[\"features.0.weight\"]\n",
    "\n",
    "# Check whether weights have successfully been assigned\n",
    "print(\n",
    "    \"Weights identical: \"\n",
    "    f\"{torch.all(vgg11.state_dict()['features.0.weight'] == vgg11_weights_dict['features.0.weight'])}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
